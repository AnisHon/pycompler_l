import re

from lex.lexer import Lexer


class Token:
    def __init__(self, kind, value, line_num):
        self.kind = kind
        self.value = value
        self.line_num = line_num

    def __str__(self):
        return f"[{self.line_num}]{self.kind}( {self.value} )"

    def __repr__(self):
        return f"\"{self.__str__()}\""

# å®šä¹‰è¯æ³•è§„åˆ™ï¼ˆæ­£åˆ™è¡¨è¾¾å¼ï¼‰
token_spec = [
    # å…³é”®å­—ï¼ˆå¿…é¡»æ”¾åœ¨æ ‡è¯†ç¬¦å‰ï¼‰
    ('KEYWORD', r'int|float|char|if|else|while|return'),
    # æ•°å­—ï¼ˆæ•´æ•°/æµ®ç‚¹æ•°ï¼‰
    ('NUMBER', r'[0-9]+\.[0-9]+|[0-9]+'),
    # è¿ç®—ç¬¦å’Œåˆ†éš”ç¬¦
    ('OP', r'[+\-*/=<>!&|^%]'),
    ('SEPARATOR', r'[(),;{}]'),
    # æ ‡è¯†ç¬¦
    ('ID', r'[a-zA-Z_][A-Za-z0-9_]*'),
    # å­—ç¬¦ä¸²å’Œå­—ç¬¦
    ('STRING', r'".*?"'),
    ('CHAR', r"'.'"),
    # æ³¨é‡Šï¼ˆç›´æ¥å¿½ç•¥ï¼‰
    # ('COMMENT', r'//.*|/\*[\s\S]*?\*/'),
    # ç©ºç™½ï¼ˆå¿½ç•¥ï¼‰
    # ('WHITESPACE', r'\s+'),
    # é¢„å¤„ç†æŒ‡ä»¤
    # ('PREPROCESSOR', r'#.*'),
    # é”™è¯¯å¤„ç†
    ('ERROR', r'.')
]

lex = Lexer(token_spec)

print(len(lex.dfa.nodes))
print(lex.dfa.edges.__len__())


state = lex.origin
for i in '"+ / * -ä½ å¥½ä¸–ç•Œ ğŸ–•ï¸114514 \\"':
    i = lex.dfa.range_map.search(i).meta
    state = lex.dfa.translate_to(state, i)
    print(lex.dfa.nodes[state])
